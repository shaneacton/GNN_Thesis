{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1904.02342.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* extracts entity, coreference, and relation annotations for each text with a stateof-the-art information extraction system (Luan et al., 2018) called  Scientific Information Extractor (SCIIE)\n",
    "* represents the annotations as a knowledge graph which collapses co-referential entity nodes\n",
    "* Authors claim the GNN is most similar to Graph Attention Network (GAT) Velickovic et al. (2018)\n",
    "* Transformer models use multiheaded attention, which allows the self attention mechanism to focus on multiple types of things at once. Once trained, each head abstractly represents attention which looks for differing properties, in this way each head can specialise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
