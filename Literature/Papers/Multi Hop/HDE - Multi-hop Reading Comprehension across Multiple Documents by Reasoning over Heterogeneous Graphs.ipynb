{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets: Wikihop (70.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-attention: \n",
    "* Co-attention has achieved great\n",
    "success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016),\n",
    "and recently was applied to multiple-hop reading comprehension (Zhong et al., 2019). Coattention enables the model to combine learned\n",
    "query contextual information attended by document and document contextual information attended by query, with inputs of one query and one\n",
    "document. We follow the implementation of coattention in (Zhong et al., 2019).\n",
    "\n",
    "* We expect The resulting attended sequence carries query-aware contextual\n",
    "information of supporting documents as shown by\n",
    "Zhong et al. (2019)\n",
    "\n",
    "# Self-attentive pooling: \n",
    "* while co-attention\n",
    "yields a query-aware contextual representation of\n",
    "documents, self-attentive pooling is designed to\n",
    "    convert the sequential contextual representation to\n",
    "a fixed dimensional non-sequential feature vector by selecting important query-aware information (Zhong et al., 2019). Self-attentive pooling\n",
    "summarizes the information presented in the coattention output by calculating a score for each\n",
    "word in the sequence. The scores are normalized\n",
    "and a weighted sum based pooling is applied to\n",
    "the sequence to get a single feature vector as the\n",
    "summarization of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a document representation\n",
    "HDE first uses coattention to transform the document sequence into a query aware document sequence, then uses self attention to do a weighted sum over the document sequence to reduce it to one element. Here n-multiheaded attention may be used to produce an element which is n times bigger than the original embeddings by concatenating the n self-attended sums. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
