Todo:

* rewrite:
    * edge type embs/ param switching
    * use non switch coattention
    * switch to graph embedder instead of doing it in HDEModel
    * switch timing to wandb logging over prints

* test edge embeddings with B0

* Untested differences between ours and HDE
    * type embs in normal summariser

* General:
    * add dropout to output model and other
    * try different learning rates for transformer/gnn
    * stop training run if it is removed from the run schedule

* Transformers
    * more batching can be done in the summariser
    * code reuse between switch and regular variants

* GNN's.
    *Try residual layer for GATConv

* Fixes
    * fix single run function
    * fix local loss viz


* house keeping:
    * ssh keygen for viz
    * move wandb folder out of HDE
    * get rid of the summariser/switch summariser distinction
        turn rel off by only using the global edge


* Visualisations
    * draw attention graphs, showing inner workings of attention heads
        * could graph the aggregate flow of info through the different node/ege types
            * aggregate across all instances, all layers, all nodes
    * graph memory usage/ flops/ runtime   against   sequence length and sparsity

* Graph structuring
    * make adding new node types easy
    * sentence nodes
        * different connection possibilities. hierarchical, sequential, codocument, complement
    * query node connected to all other nodes
        * one for each context-aware query sequence. one untransformed
    * make new config system for graph structuring options
    * resolve passage vs document naming
    * new git branch for the new graphing system
    * graph construction statistics on wandb
        * num nodes, num edges, num discarded examples, num_fully_connected edges, entropy
        * num types of nodes and edges
        * add network timing data to wandb

* Perceiver GNN
    * byte array is the (E,F) edge message vector.
    * latent array could the (N,F) node vectors, or a std perceiver latent emb

* Pretraining
    * Entities are masked from the system


Installs:
for pytorch geometric you need to install cudann
also: sudo apt-get install aptitude && sudo aptitude install build-essential

ENV_NAME=gnn_env
conda create --name ${ENV_NAME} python=3.8
y
conda activate ${ENV_NAME}

CUDA=cu101
TORCH=1.7.0
conda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=10.1 -c pytorch
conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch

pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-geometric --user
pip install transformers --user
conda install -c conda-forge nlp
# token only models do not need spacy
pip install -U spacy==2.1.3 --user
python -m spacy download en_core_web_sm
# NC not needed unless corefs are used as nodes/edges
pip install neuralcoref --no-binary neuralcoref --user
# for vizualisation capabilities
conda install -c conda-forge python-graphviz
# for wandb logging
pip install wandb


