Todo:
1: refactor token sequence, move functions to utils
1: remove span information such as entities and sentences from token sequence.
 1.1: move this info into the context graph
2: develop hierarchy in context graph where L0 is tokens, L1 is objects eg entities, L2 is sentences, L3 is passages, L4 is documents
 2.1: abstract sequential, and window edges to operate on L0,1,2
3: store token sequence in context graph
4: implement context graph gnn
4: graph config json to define context graph construction methods
5: start with live conversions - work on saving later

1: save context graphs instead of datapoints
 1.1: have GraphDataset load up context graphs and convert them into geometric datapoints on the fly
 1.2: during datapoint creation, sequence summarisation can be done online to make state sizes consistent across graphs
2. implement online summarisation system
 2.2 seq+query coattention then self attentive-pooling can summarise docs in query aware way
 2.3 full transformer models can be used for doc summarisation
3. rework state system
 3.1 state could be implemented via a multiplexer graph layer which stores every state mat using {state_name: vec} map
 3.2 state communications could be defined eg current_state, starting_state -> current_state
 3.3 acts as a graph where communication is edges and states are nodes
 3.4 concat is used for state combination when multiple inputs are detected
4: expand graph construction
 4.1 add support for candidate and query entity nodes / graph construction
 4.2 swap out spacy coref for something better
 4.3 hierarchical unique entity nodes
 4.4 AMR
5: Make bert tokeniser cased
6: add in returning edges for the directed edges
7: come up with system to allow for grouping certain edge types under 1 relational layer.
 8.1 edge groups should behave as single edge type
 8.2 another option to have split groups on subtype/direction
 8.3 feature where subtypes are ignored by type splitting, but fed to gnns as edge weights
 8.4 option to add universal message function which is used in tandem with edge type specific functions
 7.5 closest node window edge vs full window edge
9: map back to node/edge type during gnn passes - save type:id mappings per datsaset
 9.1: node type switches update function
 9.2: edge type switches message function
10: Edge weights which are fed into the message passing via concatenation as other state such as query.



NLP Libs:
Spacy - ineffective
StanfordNLP python wrapper has no coref

