Todo:
1: Create Graph embedder
 1.1 should contain the trainable weights for document summarisers
 1.2 should contain the weights for the contextual encoder so the embedder can be shipped without needing dependancies
 1.3 make fine tuning the contextual embedder optional
 1.4 query aware embeddings
 1.5 self attentive-pooling to summarise docs/passages/sents

1: expand graph construction
 1.1 add support for candidate entity nodes
 1.2 swap out spacy coref for something better
 1.4 AMR
 1.5 decouple with spacy tokens


4: implement context graph gnn
4: graph config json to define context graph construction methods


5: Make bert tokeniser cased
6: add in returning edges for the directed edges
7: come up with system to allow for grouping certain edge types under 1 relational layer.
 8.1 edge groups should behave as single edge type
 8.2 another option to have split groups on subtype/direction
 8.3 feature where subtypes are ignored by type splitting, but fed to gnns as edge weights
 8.4 option to add universal message function which is used in tandem with edge type specific functions
 7.5 closest node window edge vs full window edge
9: map back to node/edge type during gnn passes - save type:id mappings per datsaset
 9.1: node type switches update function
 9.2: edge type switches message function
10: Edge weights which are fed into the message passing via concatenation as other state such as query.



NLP Libs:
Spacy - ineffective
StanfordNLP python wrapper has no coref

