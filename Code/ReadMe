Todo:

* git
    * move back to main branch
    * new branch for the new graphing system

* General:
    * add option to use query-coattention on the entity and candidate embeddings
    * add dropout to output model and other
    * try attentional gating as used in Hierarchical Graph Network
    * try different learning rates for transformer/gnn
    * stop training run if it is removed from the run schedule

* Transformers
    * try add query tokens to summariser sequences
    * more batching can be done in the summariser
    * code reuse between switch and regular variants
    * add cls token to regular summariser

* GNNs
    * try add output projection to custom GAT
    * directionality
    * try a hybrid layerwise and weight sharing system which wraps a GNN layer.
        * every layers, node states are updated as a function of a layerwise and an alayoral gnn


* Graph structuring
    * make adding new node types easy
    * sentence nodes
        * different connection possibilities. hierarchical, sequential, codocument, complement
    * query node connected to all other nodes
        * one for each context-aware query sequence. one untransformed
    * make new config system for graph structuring options
    * resolve passage vs document naming


* Fixes
    * fix single run function
    * fix local loss viz


* before next experiment round:
    * graph construction statistics on wandb
        * num nodes, num edges, num discarded examples, num_fully_connected edges, entropy
        * num types of nodes and edges
        * add network timing data to wandb
    * cls token in summriser

* house keeping:
    * ssh keygen for viz
    * move wandb folder out of HDE
    * get rid of the summariser/switch summariser distinction
        turn rel off by only using the global edge


* Visualisations
    * draw attention graphs, showing inner workings of attention heads
        * could graph the aggregate flow of info through the different node/ege types
            * aggregate across all instances, all layers, all nodes
    * graph memory usage/ flops/ runtime   against   sequence length and sparsity


* Perceiver GNN
    * byte array is the (E,F) edge message vector.
    * latent array could the (N,F) node vectors, or a std perceiver latent emb


Installs:
ENV_NAME=gnn_env
conda create --name ${ENV_NAME} python=3.7
y
conda activate ${ENV_NAME}

CUDA=cu102
TORCH=1.6.0
conda install pytorch==${TORCH} torchvision==0.7.0 cudatoolkit=10.2 -c pytorch
pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html --user --no-cache-dir
pip install torch-geometric --user
pip install transformers --user
conda install -c conda-forge nlp
# token only models do not need spacy
pip install -U spacy==2.1.3 --user
python -m spacy download en_core_web_sm
# NC not needed unless corefs are used as nodes
pip install neuralcoref --no-binary neuralcoref --user
# for vizualisation capabilities
conda install -c anaconda python-graphviz
# for wandb logging
pip install wandb


