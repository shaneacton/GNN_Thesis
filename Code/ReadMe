Todo:
1: Create Graph embedder
 1.1 should contain the trainable weights for document summarisers
 1.2 should contain the weights for the contextual encoder so the embedder can be shipped without needing dependancies
 1.3 make fine tuning the contextual embedder optional
2: Give document constructor ability to create token nodes

3: store token span hierarchy in context graph
4: implement context graph gnn
4: graph config json to define context graph construction methods
5: start with live conversions - work on saving later

2. implement online summarisation system
 2.2 seq+query coattention then self attentive-pooling can summarise docs in query aware way
 2.3 full transformer models can be used for doc summarisation
3. rework state system
 3.1 state could be implemented via a multiplexer graph layer which stores every state mat using {state_name: vec} map
 3.2 state communications could be defined eg current_state, starting_state -> current_state
 3.3 acts as a graph where communication is edges and states are nodes
 3.4 concat is used for state combination when multiple inputs are detected
4: expand graph construction
 4.1 add support for candidate and query entity nodes / graph construction
 4.2 swap out spacy coref for something better
 4.3 hierarchical unique entity nodes
 4.4 AMR
5: Make bert tokeniser cased
6: add in returning edges for the directed edges
7: come up with system to allow for grouping certain edge types under 1 relational layer.
 8.1 edge groups should behave as single edge type
 8.2 another option to have split groups on subtype/direction
 8.3 feature where subtypes are ignored by type splitting, but fed to gnns as edge weights
 8.4 option to add universal message function which is used in tandem with edge type specific functions
 7.5 closest node window edge vs full window edge
9: map back to node/edge type during gnn passes - save type:id mappings per datsaset
 9.1: node type switches update function
 9.2: edge type switches message function
10: Edge weights which are fed into the message passing via concatenation as other state such as query.



NLP Libs:
Spacy - ineffective
StanfordNLP python wrapper has no coref

